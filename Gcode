# tableau_inventory_fast_full.py
import os, sys, time, subprocess
from datetime import datetime
from typing import Iterable, Tuple

import pandas as pd
import tableauserverclient as TSC

# ===== EDIT THESE =====
SERVER      = "https://mytableau.cvs.com"   # no trailing '/#/'
SITE_ID     = "RX_OPS"                      # site content url (what appears after /site/)
PAT_NAME    = "xxxxx"
PAT_SECRET  = "xxxxxxxxxx"

OUTPUT_CSV        = "tableau_inventory_newdata.csv"
PAGE_SIZE         = 500                     # 500 is a good balance for TSC
MAX_RETRIES       = 4
RETRY_WAIT_SEC    = 3
# ======================


def link_for_view(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/views/{content_url}"

def link_for_workbook(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/workbooks/{content_url}"

def with_retries(fn, *args, **kwargs):
    """Run TSC call with retries on transient failures/timeouts."""
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            last_err = e
            # backoff
            time.sleep(RETRY_WAIT_SEC * attempt)
    raise last_err

def paged(getter, req_options) -> Iterable[Tuple[Iterable, TSC.PaginationItem]]:
    """Yield all pages for a TSC collection getter (projects, workbooks, views‚Ä¶)."""
    while True:
        items, page = with_retries(getter, req_options=req_options)
        yield items, page
        if page.page_number * page.page_size >= page.total_available:
            break
        req_options.page_number = page.page_number + 1

def main():
    print("="*70)
    print("üöÄ TABLEAU INVENTORY (All Projects ‚Ä¢ Fast ‚Ä¢ Resilient)")
    print("="*70)
    start = datetime.now()
    print(f"‚è∞ Start: {start:%Y-%m-%d %H:%M:%S}\n")

    server = TSC.Server(SERVER, use_server_version=True)
    auth   = TSC.PersonalAccessTokenAuth(PAT_NAME, PAT_SECRET, SITE_ID)

    rows = []
    projects_index = {}

    with server.auth.sign_in(auth):
        print(f"‚úÖ Connected to {SERVER}")

        # ---------------- Projects ----------------
        print("\nüìÅ Fetching ALL projects (top-level + nested)‚Ä¶")
        preq = TSC.RequestOptions()
        preq.page_size = PAGE_SIZE

        proj_count = 0
        for projects, page in paged(server.projects.get, preq):
            for p in projects:
                projects_index[p.id] = p
                proj_count += 1
                # Emit a row RIGHT AWAY so projects with no workbooks still appear
                rows.append({
                    "project_name":     p.name,
                    "project_id":       p.id,
                    "project_parent":   getattr(p, "parent_id", None),
                    "workbook_name":    None,
                    "workbook_id":      None,
                    "workbook_url":     None,
                    "workbook_created": None,
                    "workbook_updated": None,
                    "owner_id":         None,
                    "view_name":        None,
                    "view_id":          None,
                    "view_url":         None
                })
            print(f"   ‚Ä¶ {min(proj_count, page.total_available)}/{page.total_available} projects", end="\r")
        print(f"\n‚úÖ Projects fetched: {proj_count:,}")

        # ---------------- Workbooks by project ----------------
        print("\nüìö Fetching workbooks per project‚Ä¶ (skips heavy connection hydration)")
        # light field selection keeps responses small (supported by TSC)
        base_fields = ["id", "name", "contentUrl", "projectId", "createdAt", "updatedAt", "ownerId"]

        total_wb = 0
        for pid, proj in projects_index.items():
            wb_req = TSC.RequestOptions()
            wb_req.page_size = PAGE_SIZE
            # filter by project
            wb_req.filter.add(TSC.Filter(TSC.RequestOptions.Field.ProjectId,
                                         TSC.RequestOptions.Operator.Equals, pid))
            # ask only for light fields if available
            try:
                wb_req.fields = ",".join(base_fields)
            except Exception:
                pass

            project_wb_count = 0
            for wbs, wpage in paged(server.workbooks.get, wb_req):
                for wb in wbs:
                    total_wb += 1
                    project_wb_count += 1

                    wb_url = link_for_workbook(SERVER, SITE_ID, wb.content_url)

                    # add a row even if no views are present
                    rows.append({
                        "project_name":     proj.name,
                        "project_id":       proj.id,
                        "project_parent":   getattr(proj, "parent_id", None),
                        "workbook_name":    wb.name,
                        "workbook_id":      wb.id,
                        "workbook_url":     wb_url,
                        "workbook_created": getattr(wb, "created_at", None),
                        "workbook_updated": getattr(wb, "updated_at", None),
                        "owner_id":         getattr(wb, "owner_id", None),
                        "view_name":        None,
                        "view_id":          None,
                        "view_url":         None
                    })

                    # fetch views for this workbook (no usage stats; fast)
                    try:
                        with_retries(server.workbooks.populate_views, wb)
                        for v in getattr(wb, "views", []) or []:
                            v_url = link_for_view(SERVER, SITE_ID, v.content_url)
                            rows.append({
                                "project_name":     proj.name,
                                "project_id":       proj.id,
                                "project_parent":   getattr(proj, "parent_id", None),
                                "workbook_name":    wb.name,
                                "workbook_id":      wb.id,
                                "workbook_url":     wb_url,
                                "workbook_created": getattr(wb, "created_at", None),
                                "workbook_updated": getattr(wb, "updated_at", None),
                                "owner_id":         getattr(wb, "owner_id", None),
                                "view_name":        v.name,
                                "view_id":          v.id,
                                "view_url":         v_url
                            })
                    except Exception:
                        # If views cannot be populated, we still keep the workbook row above.
                        pass

                print(f"   {proj.name[:40]:<40} ‚Äì {project_wb_count} workbooks", end="\r")
            print(f"   {proj.name[:40]:<40} ‚Äì {project_wb_count} workbooks")

        print(f"‚úÖ Total workbooks fetched: {total_wb:,}")

    # ---------------- Save ----------------
    print("\nüíæ Saving CSV‚Ä¶")
    df = pd.DataFrame(rows)
    if df.empty:
        print("‚ùå No data extracted.")
        return

    # Order for easy filtering (Projects ‚Üí Workbooks ‚Üí Views)
    order_cols = [
        "project_name","project_id","project_parent",
        "workbook_name","workbook_id","workbook_url","workbook_created","workbook_updated","owner_id",
        "view_name","view_id","view_url"
    ]
    for c in order_cols:
        if c not in df.columns:  # guard
            df[c] = None

    df = df[order_cols].sort_values(["project_name","workbook_name","view_name"], na_position="last")
    out_path = os.path.abspath(OUTPUT_CSV)
    df.to_csv(out_path, index=False)

    # quick summary
    projects_n  = df["project_id"].nunique()
    workbooks_n = df["workbook_id"].nunique()
    views_n     = df["view_id"].nunique()
    dur = datetime.now() - start

    print("\nüéâ Done!")
    print(f"   üóÇÔ∏è Projects:  {projects_n:,}")
    print(f"   üìö Workbooks: {workbooks_n:,}")
    print(f"   üìÑ Views:     {views_n:,}")
    print(f"   ‚è±Ô∏è Duration:  {dur}")
    print(f"   üìÅ Output:    {out_path}")
