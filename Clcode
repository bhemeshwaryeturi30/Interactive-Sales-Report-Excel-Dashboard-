# optimized_tableau_inventory_extractor.py
import os
import sys
import subprocess
import pandas as pd
import tableauserverclient as TSC
from datetime import datetime
import time
import logging

# ===== EDIT THESE =====
SERVER      = "https://mytableau.cvs.com"   # Root URL (no '/#/')
SITE_ID     = "RX_OPS"                      # e.g., RX_OPS
PAT_NAME    = "xxxxx"
PAT_SECRET  = "xxxxxxxxxx"

OUTPUT_CSV        = "tableau_inventory_newdata.csv"
DOWNLOAD_VIEW_CSV = False                   # True = export underlying data CSVs for each view (needs permission)
VIEW_CSV_DIR      = "view_csvs"
PAGE_SIZE         = 100                     # Reduced from 1000 to avoid timeouts
MAX_RETRIES       = 3                       # Number of retries for failed requests
RETRY_DELAY       = 2                       # Seconds to wait between retries
# ======================

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def print_progress_bar(current, total, prefix='Progress', suffix='Complete', length=50, fill='█'):
    """Print a progress bar to terminal"""
    if total == 0:
        return
    percent = f"{100 * (current / float(total)):.1f}"
    filled_length = int(length * current // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix} ({current}/{total})', end='', flush=True)
    if current == total:
        print()  # New line when complete

def link_for_view(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/views/{content_url}"

def link_for_workbook(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/workbooks/{content_url}"

def has_more(pagination):
    # TSC pagination: page_number (1-based), page_size, total_available
    return pagination.page_number * pagination.page_size < pagination.total_available

def retry_request(func, *args, **kwargs):
    """Retry a request with exponential backoff"""
    for attempt in range(MAX_RETRIES):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == MAX_RETRIES - 1:
                logger.error(f"Failed after {MAX_RETRIES} attempts: {e}")
                raise
            wait_time = RETRY_DELAY * (2 ** attempt)
            logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
            time.sleep(wait_time)

def open_excel(filepath):
    """Force open a file in Excel (Windows). Falls back to default opener on other OS."""
    try:
        if sys.platform.startswith("win"):
            subprocess.Popen(["excel.exe", filepath], shell=True)
        elif sys.platform == "darwin":
            subprocess.Popen(["open", "-a", "Microsoft Excel", filepath])
        else:
            subprocess.Popen(["xdg-open", filepath])
    except Exception as e:
        print(f"⚠️ Could not open in Excel: {e}")
        try:
            if sys.platform.startswith("win"):
                subprocess.Popen(["start", "", filepath], shell=True)
            elif sys.platform == "darwin":
                subprocess.Popen(["open", filepath])
            else:
                subprocess.Popen(["xdg-open", filepath])
        except Exception as e2:
            print(f"⚠️ Fallback open failed: {e2}")

def open_folder(path):
    try:
        if sys.platform.startswith("win"):
            subprocess.Popen(["explorer", path])
        elif sys.platform == "darwin":
            subprocess.Popen(["open", path])
        else:
            subprocess.Popen(["xdg-open", path])
    except Exception as e:
        print(f"⚠️ Could not open folder: {e}")

def get_total_workbooks(server):
    """Get total count of workbooks for progress calculation"""
    try:
        req = TSC.RequestOptions()
        req.page_size = 1  # Just get count, not data
        _, pagination = retry_request(server.workbooks.get, req_options=req)
        return pagination.total_available
    except Exception as e:
        logger.error(f"Could not get workbook count: {e}")
        return 0

def fetch_all_projects(server):
    """Fetch ALL projects, not just top-level ones"""
    print("📁 Fetching ALL projects (including nested ones)...")
    projects_by_id = {}
    
    # Create request options for ALL projects (remove any filters)
    preq = TSC.RequestOptions()
    preq.page_size = PAGE_SIZE
    # Don't set any filters - we want ALL projects
    
    project_count = 0
    page_number = 1
    
    while True:
        try:
            preq.page_number = page_number
            projects, p_page = retry_request(server.projects.get, req_options=preq)
            
            for p in projects:
                projects_by_id[p.id] = p
                project_count += 1
                logger.debug(f"Found project: {p.name} (ID: {p.id})")
            
            if p_page.total_available > 0:
                print_progress_bar(project_count, p_page.total_available, 
                                 prefix='Projects', suffix='fetched')
            
            if not has_more(p_page):
                break
                
            page_number += 1
            
        except Exception as e:
            logger.error(f"Error fetching projects page {page_number}: {e}")
            break
    
    print(f"✅ Fetched {len(projects_by_id):,} projects")
    return projects_by_id

def fetch_datasources_safe(server):
    """Fetch datasources with better error handling"""
    print("💾 Fetching datasources...")
    ds_by_id = {}
    
    try:
        dreq = TSC.RequestOptions()
        dreq.page_size = PAGE_SIZE
        ds_count = 0
        page_number = 1
        
        while True:
            try:
                dreq.page_number = page_number
                datasources, d_page = retry_request(server.datasources.get, req_options=dreq)
                
                for d in datasources:
                    ds_by_id[d.id] = d
                    ds_count += 1
                
                if d_page.total_available > 0:
                    print_progress_bar(ds_count, d_page.total_available, 
                                     prefix='Datasources', suffix='fetched')
                
                if not has_more(d_page):
                    break
                    
                page_number += 1
                
            except Exception as e:
                logger.warning(f"Error fetching datasources page {page_number}: {e}")
                break
                
    except Exception as e:
        logger.warning(f"Could not fetch datasources: {e}")
        ds_by_id = {}
    
    print(f"✅ Fetched {len(ds_by_id):,} datasources")
    return ds_by_id

def save_partial_results(rows, output_path):
    """Save partial results in case of interruption"""
    if not rows:
        return
        
    df = pd.DataFrame(rows)
    backup_path = output_path.replace('.csv', '_partial_backup.csv')
    df_sorted = df.sort_values(
        ["project_name", "workbook_name", "view_name"], na_position="last"
    )
    df_sorted.to_csv(backup_path, index=False)
    print(f"💾 Partial backup saved to: {backup_path}")

def main():
    print("="*60)
    print("🚀 TABLEAU INVENTORY EXTRACTION (OPTIMIZED)")
    print("="*60)
    start_time = datetime.now()
    print(f"⏰ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Configure server with better timeout settings
    server = TSC.Server(SERVER, use_server_version=True)
    server.http_options['verify'] = True  # Ensure SSL verification
    auth = TSC.PersonalAccessTokenAuth(PAT_NAME, PAT_SECRET, SITE_ID)

    rows = []
    processed_workbooks = 0

    try:
        with server.auth.sign_in(auth):
            print(f"✅ Successfully connected to {SERVER}")
            print()

            # ---- Get total workbook count first ----
            print("🔍 Getting total workbook count...")
            total_workbooks = get_total_workbooks(server)
            print(f"📊 Total workbooks to process: {total_workbooks:,}")
            print()

            # ---- Fetch ALL projects (not just top-level) ----
            projects_by_id = fetch_all_projects(server)
            print()

            # ---- Fetch datasources with better error handling ----
            ds_by_id = fetch_datasources_safe(server)
            print()

            # ---- Skip view usage stats initially to speed up processing ----
            # We'll get this during workbook processing if needed
            print("⏩ Skipping pre-loading view usage stats for faster processing...")
            print()

            # ---- Process workbooks with better pagination ----
            print("="*60)
            print("📚 PROCESSING WORKBOOKS & VIEWS")
            print("="*60)
            
            wb_req = TSC.RequestOptions()
            wb_req.page_size = PAGE_SIZE
            page_number = 1
            
            while True:
                try:
                    wb_req.page_number = page_number
                    workbooks, w_page = retry_request(server.workbooks.get, req_options=wb_req)

                    for wb in workbooks:
                        processed_workbooks += 1
                        
                        # Update progress bar
                        if total_workbooks > 0:
                            print_progress_bar(processed_workbooks, total_workbooks, 
                                             prefix='📚 Workbooks', suffix='processed')
                        
                        # Log every 10 workbooks (more frequent)
                        if processed_workbooks % 10 == 0:
                            print(f"\n📈 Progress: {processed_workbooks:,}/{total_workbooks:,} workbooks "
                                  f"({100*processed_workbooks/total_workbooks:.1f}%) - Current: {wb.name[:50]}...")
                            # Save partial backup every 50 workbooks
                            if processed_workbooks % 50 == 0:
                                save_partial_results(rows, os.path.abspath(OUTPUT_CSV))

                        # Process workbook with error handling
                        try:
                            server.workbooks.populate_views(wb)
                        except Exception as e:
                            logger.warning(f"Could not populate views for workbook {wb.name}: {e}")
                            wb.views = []

                        try:
                            server.workbooks.populate_connections(wb)
                        except Exception as e:
                            logger.warning(f"Could not populate connections for workbook {wb.name}: {e}")
                            wb.connections = []

                        proj = projects_by_id.get(wb.project_id)
                        wb_url = link_for_workbook(SERVER, SITE_ID, wb.content_url)

                        # Datasource information
                        ds_names, ds_types = [], []
                        for c in getattr(wb, "connections", []):
                            ds_obj = ds_by_id.get(getattr(c, "datasource_id", None))
                            if ds_obj:
                                ds_names.append(ds_obj.name or "")
                                ds_types.append(getattr(ds_obj, "connection_type", "") or "")
                            else:
                                ds_names.append(getattr(c, "server_address", "") or "embedded/unknown")
                                ds_types.append(getattr(c, "connection_type", "") or "unknown")

                        # Process views
                        views = getattr(wb, "views", [])
                        if not views:
                            # Add workbook row even if no views
                            rows.append({
                                "project_name":      proj.name if proj else "Unknown",
                                "project_id":        wb.project_id,
                                "workbook_name":     wb.name,
                                "workbook_id":       wb.id,
                                "workbook_url":      wb_url,
                                "workbook_created":  getattr(wb, "created_at", None),
                                "workbook_updated":  getattr(wb, "updated_at", None),
                                "owner_id":          getattr(wb, "owner_id", None),
                                "view_name":         None,
                                "view_id":           None,
                                "view_url":          None,
                                "view_total_views":  None,
                                "datasource_names":  "; ".join(sorted(set(n for n in ds_names if n))),
                                "datasource_types":  "; ".join(sorted(set(t for t in ds_types if t))),
                                "saved_view_csv":    None
                            })
                        else:
                            for v in views:
                                v_url = link_for_view(SERVER, SITE_ID, v.content_url)
                                
                                # Get view usage stats on-demand (safer)
                                total_views = None
                                try:
                                    total_views = getattr(v, "total_views", None)
                                except:
                                    pass

                                saved_csv_path = None
                                if DOWNLOAD_VIEW_CSV:
                                    try:
                                        os.makedirs(VIEW_CSV_DIR, exist_ok=True)
                                        csv_bytes = server.views.populate_csv(v)
                                        safe = f"{(proj.name if proj else 'NoProject')}__{wb.name}__{v.name}".replace("/", "_").replace("\\", "_")
                                        saved_csv_path = os.path.join(VIEW_CSV_DIR, f"{safe}.csv")
                                        with open(saved_csv_path, "wb") as f:
                                            f.write(csv_bytes)
                                    except Exception as e:
                                        saved_csv_path = f"ERROR: {e}"

                                rows.append({
                                    "project_name":      proj.name if proj else "Unknown",
                                    "project_id":        wb.project_id,
                                    "workbook_name":     wb.name,
                                    "workbook_id":       wb.id,
                                    "workbook_url":      wb_url,
                                    "workbook_created":  getattr(wb, "created_at", None),
                                    "workbook_updated":  getattr(wb, "updated_at", None),
                                    "owner_id":          getattr(wb, "owner_id", None),
                                    "view_name":         v.name,
                                    "view_id":           v.id,
                                    "view_url":          v_url,
                                    "view_total_views":  total_views,
                                    "datasource_names":  "; ".join(sorted(set(n for n in ds_names if n))),
                                    "datasource_types":  "; ".join(sorted(set(t for t in ds_types if t))),
                                    "saved_view_csv":    saved_csv_path
                                })

                    if not has_more(w_page):
                        break
                        
                    page_number += 1
                    
                except Exception as e:
                    logger.error(f"Error processing workbooks page {page_number}: {e}")
                    break

    except KeyboardInterrupt:
        print(f"\n\n⚠️ Extraction interrupted by user!")
        print(f"📊 Processed {processed_workbooks:,} workbooks so far")
        if len(rows) > 0:
            print("💾 Saving partial results...")
            save_partial_results(rows, os.path.abspath(OUTPUT_CSV))
        else:
            print("❌ No data to save")
            return

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        if len(rows) > 0:
            print("💾 Saving partial results due to error...")
            save_partial_results(rows, os.path.abspath(OUTPUT_CSV))
        raise

    # ---- Save final results ----
    print(f"\n\n" + "="*60)
    print("💾 SAVING FINAL RESULTS")
    print("="*60)
    
    output_path = os.path.abspath(OUTPUT_CSV)
    df = pd.DataFrame(rows)
    
    if len(df) > 0:
        df_sorted = df.sort_values(
            ["project_name", "workbook_name", "view_name"], na_position="last"
        )
        df_sorted.to_csv(output_path, index=False)

        # Summary statistics
        total_projects = df['project_id'].nunique()
        total_workbooks_found = df['workbook_id'].nunique()
        total_views = len(df[df['view_name'].notna()])
        
        end_time = datetime.now()
        duration = end_time - start_time

        print("="*60)
        print("🎉 EXTRACTION COMPLETED!")
        print("="*60)
        print(f"📊 SUMMARY:")
        print(f"   🗂️  Projects: {total_projects:,}")
        print(f"   📚 Workbooks: {total_workbooks_found:,}")
        print(f"   📄 Views: {total_views:,}")
        print(f"   ⏱️  Duration: {duration}")
        print(f"   📁 Output: {output_path}")

        if DOWNLOAD_VIEW_CSV:
            print(f"   📂 Per-view CSVs: {os.path.abspath(VIEW_CSV_DIR)}")

        print(f"\n🚀 Opening Excel...")
        open_excel(output_path)
        if DOWNLOAD_VIEW_CSV:
            open_folder(os.path.abspath(VIEW_CSV_DIR))
    else:
        print("❌ No data was extracted!")

if __name__ == "__main__":
    main()
