# tableau_inventory_optimized.py
import os
import sys
import subprocess
import pandas as pd
import tableauserverclient as TSC
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading
import time

# ===== EDIT THESE =====
SERVER      = "https://mytableau.cvs.com"   # Root URL (no '/#/')
SITE_ID     = "RX_OPS"                      # e.g., RX_OPS
PAT_NAME    = "xxxxx"
PAT_SECRET  = "xxxxxxxxxx"

OUTPUT_CSV        = "tableau_inventory_newdata.csv"
DOWNLOAD_VIEW_CSV = False                   # True = export underlying data CSVs for each view (needs permission)
VIEW_CSV_DIR      = "view_csvs"
PAGE_SIZE         = 1000                    # Max page size for TSC
MAX_WORKERS       = 8                       # Number of concurrent threads
BATCH_SIZE        = 50                      # Process workbooks in batches
# ======================

class TableauInventoryExtractor:
    def __init__(self):
        self.server = TSC.Server(SERVER, use_server_version=True)
        self.auth = TSC.PersonalAccessTokenAuth(PAT_NAME, PAT_SECRET, SITE_ID)
        self.lock = threading.Lock()
        self.rows = []
        self.processed_count = 0
        self.error_count = 0
        
        # Pre-loaded data
        self.projects_by_id = {}
        self.ds_by_id = {}
        self.view_usage_by_id = {}

    def print_progress_bar(self, current, total, prefix='Progress', suffix='Complete', length=50, fill='‚ñà'):
        """Print a progress bar to terminal"""
        if total == 0:
            return
        percent = f"{100 * (current / float(total)):.1f}"
        filled_length = int(length * current // total)
        bar = fill * filled_length + '-' * (length - filled_length)
        print(f'\r{prefix} |{bar}| {percent}% {suffix} ({current}/{total})', end='', flush=True)
        if current == total:
            print()

    def link_for_view(self, content_url):
        return f"{SERVER}/#/site/{SITE_ID}/views/{content_url}"

    def link_for_workbook(self, content_url):
        return f"{SERVER}/#/site/{SITE_ID}/workbooks/{content_url}"

    def has_more(self, pagination):
        return pagination.page_number * pagination.page_size < pagination.total_available

    def load_all_projects(self):
        """Load all projects in one go with optimized pagination"""
        print("üìÅ Fetching all projects...")
        req = TSC.RequestOptions()
        req.page_size = PAGE_SIZE
        project_count = 0
        
        while True:
            projects, page = self.server.projects.get(req_options=req)
            for p in projects:
                self.projects_by_id[p.id] = p
                project_count += 1
            
            if page.total_available > 0:
                self.print_progress_bar(project_count, page.total_available, 
                                      prefix='Projects', suffix='fetched')
            
            if not self.has_more(page):
                break
            req.page_number = page.page_number + 1
        
        print(f"‚úÖ Loaded {len(self.projects_by_id):,} projects")

    def load_all_datasources(self):
        """Load all datasources with error handling"""
        print("üíæ Fetching all datasources...")
        try:
            req = TSC.RequestOptions()
            req.page_size = PAGE_SIZE
            ds_count = 0
            
            while True:
                datasources, page = self.server.datasources.get(req_options=req)
                for d in datasources:
                    self.ds_by_id[d.id] = d
                    ds_count += 1
                
                if page.total_available > 0:
                    self.print_progress_bar(ds_count, page.total_available, 
                                          prefix='Datasources', suffix='fetched')
                
                if not self.has_more(page):
                    break
                req.page_number = page.page_number + 1
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading datasources: {e}")
            self.ds_by_id = {}
        
        print(f"‚úÖ Loaded {len(self.ds_by_id):,} datasources")

    def load_all_views_bulk(self):
        """Load all views with usage stats in bulk - much faster than per-workbook"""
        print("üìä Fetching all views with usage statistics...")
        try:
            req = TSC.RequestOptions()
            req.page_size = PAGE_SIZE
            req.include_usage_statistics = True
            view_count = 0
            
            while True:
                views, page = self.server.views.get(req_options=req)
                for v in views:
                    self.view_usage_by_id[v.id] = {
                        'total_views': getattr(v, 'total_views', None),
                        'workbook_id': getattr(v, 'workbook_id', None),
                        'name': getattr(v, 'name', ''),
                        'content_url': getattr(v, 'content_url', ''),
                        'id': v.id
                    }
                    view_count += 1
                
                if page.total_available > 0:
                    self.print_progress_bar(view_count, page.total_available, 
                                          prefix='Views', suffix='fetched')
                
                if not self.has_more(page):
                    break
                req.page_number = page.page_number + 1
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading views: {e}")
            self.view_usage_by_id = {}
        
        print(f"‚úÖ Loaded {len(self.view_usage_by_id):,} views with usage stats")

    def get_workbook_connections(self, workbook_id):
        """Get connections for a specific workbook with retry logic"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Create a new server instance for thread safety
                thread_server = TSC.Server(SERVER, use_server_version=True)
                with thread_server.auth.sign_in(self.auth):
                    wb = thread_server.workbooks.get_by_id(workbook_id)
                    thread_server.workbooks.populate_connections(wb)
                    return getattr(wb, 'connections', [])
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"‚ö†Ô∏è Failed to get connections for workbook {workbook_id}: {e}")
                    return []
                time.sleep(0.5)  # Brief pause before retry

    def process_workbook_batch(self, workbooks_batch):
        """Process a batch of workbooks - designed for threading"""
        batch_rows = []
        
        for wb in workbooks_batch:
            try:
                # Get basic workbook info
                proj = self.projects_by_id.get(wb.project_id)
                wb_url = self.link_for_workbook(wb.content_url)
                
                # Get connections (this is the expensive part)
                connections = self.get_workbook_connections(wb.id)
                
                # Process datasource info from connections
                ds_names, ds_types = [], []
                for c in connections:
                    ds_obj = self.ds_by_id.get(getattr(c, 'datasource_id', None))
                    if ds_obj:
                        ds_names.append(ds_obj.name or "")
                        ds_types.append(ds_obj.connection_type or "")
                    else:
                        ds_names.append(getattr(c, 'server_address', '') or 'embedded/unknown')
                        ds_types.append(getattr(c, 'connection_type', '') or 'unknown')
                
                # Find views for this workbook from our pre-loaded data
                workbook_views = [v for v in self.view_usage_by_id.values() 
                                if v.get('workbook_id') == wb.id]
                
                # If no views found, create one row with workbook info
                if not workbook_views:
                    batch_rows.append({
                        "project_name": proj.name if proj else None,
                        "project_id": wb.project_id,
                        "workbook_name": wb.name,
                        "workbook_id": wb.id,
                        "workbook_url": wb_url,
                        "workbook_created": getattr(wb, 'created_at', None),
                        "workbook_updated": getattr(wb, 'updated_at', None),
                        "owner_id": getattr(wb, 'owner_id', None),
                        "view_name": None,
                        "view_id": None,
                        "view_url": None,
                        "view_total_views": None,
                        "datasource_names": "; ".join(sorted(set(n for n in ds_names if n))),
                        "datasource_types": "; ".join(sorted(set(t for t in ds_types if t))),
                        "saved_view_csv": None
                    })
                else:
                    # Create a row for each view
                    for v_info in workbook_views:
                        v_url = self.link_for_view(v_info['content_url'])
                        
                        saved_csv_path = None
                        if DOWNLOAD_VIEW_CSV:
                            # Note: This would require separate API call - disabled for performance
                            saved_csv_path = "SKIPPED_FOR_PERFORMANCE"
                        
                        batch_rows.append({
                            "project_name": proj.name if proj else None,
                            "project_id": wb.project_id,
                            "workbook_name": wb.name,
                            "workbook_id": wb.id,
                            "workbook_url": wb_url,
                            "workbook_created": getattr(wb, 'created_at', None),
                            "workbook_updated": getattr(wb, 'updated_at', None),
                            "owner_id": getattr(wb, 'owner_id', None),
                            "view_name": v_info['name'],
                            "view_id": v_info['id'],
                            "view_url": v_url,
                            "view_total_views": v_info['total_views'],
                            "datasource_names": "; ".join(sorted(set(n for n in ds_names if n))),
                            "datasource_types": "; ".join(sorted(set(t for t in ds_types if t))),
                            "saved_view_csv": saved_csv_path
                        })
                
                # Thread-safe progress update
                with self.lock:
                    self.processed_count += 1
                    
            except Exception as e:
                with self.lock:
                    self.error_count += 1
                    print(f"\n‚ö†Ô∏è Error processing workbook {wb.name}: {e}")
        
        # Thread-safe row addition
        with self.lock:
            self.rows.extend(batch_rows)
        
        return len(batch_rows)

    def get_all_workbooks(self):
        """Get all workbooks efficiently"""
        print("üìö Fetching all workbooks...")
        workbooks = []
        req = TSC.RequestOptions()
        req.page_size = PAGE_SIZE
        wb_count = 0
        
        while True:
            wb_batch, page = self.server.workbooks.get(req_options=req)
            workbooks.extend(wb_batch)
            wb_count += len(wb_batch)
            
            if page.total_available > 0:
                self.print_progress_bar(wb_count, page.total_available, 
                                      prefix='Workbooks', suffix='fetched')
            
            if not self.has_more(page):
                break
            req.page_number = page.page_number + 1
        
        print(f"‚úÖ Fetched {len(workbooks):,} workbooks")
        return workbooks

    def extract_data(self):
        """Main extraction method with optimizations"""
        print("="*60)
        print("üöÄ OPTIMIZED TABLEAU INVENTORY EXTRACTION")
        print("="*60)
        start_time = datetime.now()
        print(f"‚è∞ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            with self.server.auth.sign_in(self.auth):
                print(f"‚úÖ Successfully connected to {SERVER}")
                print()
                
                # Load all reference data first (bulk operations)
                self.load_all_projects()
                self.load_all_datasources()
                self.load_all_views_bulk()  # This is the key optimization!
                
                # Get all workbooks
                all_workbooks = self.get_all_workbooks()
                total_workbooks = len(all_workbooks)
                
                if total_workbooks == 0:
                    print("‚ùå No workbooks found!")
                    return
                
                print(f"\nüìä Processing {total_workbooks:,} workbooks with {MAX_WORKERS} threads...")
                print("="*60)
                
                # Process workbooks in batches with threading
                self.processed_count = 0
                
                # Create batches
                batches = [all_workbooks[i:i + BATCH_SIZE] 
                          for i in range(0, len(all_workbooks), BATCH_SIZE)]
                
                # Process batches concurrently
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    # Submit all batches
                    futures = [executor.submit(self.process_workbook_batch, batch) 
                              for batch in batches]
                    
                    # Monitor progress
                    completed = 0
                    while completed < len(futures):
                        completed = sum(1 for f in futures if f.done())
                        if total_workbooks > 0:
                            current_processed = min(self.processed_count, total_workbooks)
                            self.print_progress_bar(current_processed, total_workbooks, 
                                                   prefix='üìö Processing', suffix='completed')
                        time.sleep(0.5)
                    
                    # Wait for all to complete
                    for future in futures:
                        future.result()
                
                print(f"\n‚úÖ Completed processing {self.processed_count:,} workbooks")
                if self.error_count > 0:
                    print(f"‚ö†Ô∏è Encountered {self.error_count} errors")
                
        except KeyboardInterrupt:
            print(f"\n\n‚ö†Ô∏è Extraction interrupted by user!")
            print(f"üìä Processed {self.processed_count:,} workbooks so far")
        except Exception as e:
            print(f"\n‚ùå Fatal error: {e}")
            return
        
        # Save results
        self.save_results(start_time)

    def save_results(self, start_time):
        """Save results to CSV and provide summary"""
        print(f"\n" + "="*60)
        print("üíæ SAVING RESULTS")
        print("="*60)
        
        output_path = os.path.abspath(OUTPUT_CSV)
        df = pd.DataFrame(self.rows)
        
        if len(df) > 0:
            df_sorted = df.sort_values(
                ["project_name", "workbook_name", "view_name"], na_position="last"
            )
            df_sorted.to_csv(output_path, index=False)
            
            # Summary statistics
            total_projects = df['project_id'].nunique()
            total_workbooks_found = df['workbook_id'].nunique()
            total_views = df['view_id'].nunique()  # Only count non-null views
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("="*60)
            print("üéâ EXTRACTION COMPLETED!")
            print("="*60)
            print(f"üìä SUMMARY:")
            print(f"   üóÇÔ∏è  Projects: {total_projects:,}")
            print(f"   üìö Workbooks: {total_workbooks_found:,}")
            print(f"   üìÑ Views: {total_views:,}")
            print(f"   üìã Total Rows: {len(df):,}")
            print(f"   ‚è±Ô∏è  Duration: {duration}")
            print(f"   üìÅ Output: {output_path}")
            print(f"   ‚ö° Performance: {total_workbooks_found/duration.total_seconds():.2f} workbooks/second")
            
            if self.error_count > 0:
                print(f"   ‚ö†Ô∏è  Errors: {self.error_count}")
            
            # Open in Excel
            print(f"\nüöÄ Opening Excel...")
            self.open_excel(output_path)
        else:
            print("‚ùå No data was extracted!")

    def open_excel(self, filepath):
        """Open file in Excel with fallback options"""
        try:
            if sys.platform.startswith("win"):
                subprocess.Popen(["excel.exe", filepath], shell=True)
            elif sys.platform == "darwin":
                subprocess.Popen(["open", "-a", "Microsoft Excel", filepath])
            else:
                subprocess.Popen(["xdg-open", filepath])
        except Exception as e:
            print(f"‚ö†Ô∏è Could not open in Excel: {e}")
            try:
                if sys.platform.startswith("win"):
                    subprocess.Popen(["start", "", filepath], shell=True)
                elif sys.platform == "darwin":
                    subprocess.Popen(["open", filepath])
                else:
                    subprocess.Popen(["xdg-open", filepath])
            except Exception:
                print(f"‚ö†Ô∏è Could not open file automatically")

def main():
    extractor = TableauInventoryExtractor()
    extractor.extract_data()

if __name__ == "__main__":
    main()
