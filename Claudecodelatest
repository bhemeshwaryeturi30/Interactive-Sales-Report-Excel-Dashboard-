# Enhanced tableau_inventory_with_progress.py
import os
import sys
import subprocess
import pandas as pd
import tableauserverclient as TSC
from datetime import datetime

# ===== EDIT THESE =====
SERVER      = "https://mytableau.cvs.com"   # Root URL (no '/#/')
SITE_ID     = "RX_OPS"                      # e.g., RX_OPS
PAT_NAME    = "YOUR_PAT_NAME"
PAT_SECRET  = "YOUR_PAT_SECRET"

OUTPUT_CSV        = "tableau_inventory.csv"
DOWNLOAD_VIEW_CSV = False                   # True = export underlying data CSVs for each view (needs permission)
VIEW_CSV_DIR      = "view_csvs"
PAGE_SIZE         = 1000                    # Max page size for TSC
RESOLVE_OWNERS    = True                    # True = fetch user names for owner_ids
# ======================

def print_progress_bar(current, total, prefix='Progress', suffix='Complete', length=50, fill='█'):
    """Print a progress bar to terminal"""
    if total == 0:
        return
    percent = f"{100 * (current / float(total)):.1f}"
    filled_length = int(length * current // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix} ({current}/{total})', end='', flush=True)
    if current == total:
        print()  # New line when complete

def link_for_view(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/views/{content_url}"

def link_for_workbook(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/workbooks/{content_url}"

def link_for_datasource(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/datasources/{content_url}"

def has_more(pagination):
    # TSC pagination: page_number (1-based), page_size, total_available
    return pagination.page_number * pagination.page_size < pagination.total_available

def open_excel(filepath):
    """Force open a file in Excel (Windows). Falls back to default opener on other OS."""
    try:
        if sys.platform.startswith("win"):
            # Try excel.exe via PATH
            subprocess.Popen(["excel.exe", filepath], shell=True)
        elif sys.platform == "darwin":
            subprocess.Popen(["open", "-a", "Microsoft Excel", filepath])
        else:
            subprocess.Popen(["xdg-open", filepath])
    except Exception as e:
        print(f"⚠️ Could not open in Excel: {e}")
        # Fallback to default handler
        try:
            if sys.platform.startswith("win"):
                subprocess.Popen(["start", "", filepath], shell=True)
            elif sys.platform == "darwin":
                subprocess.Popen(["open", filepath])
            else:
                subprocess.Popen(["xdg-open", filepath])
        except Exception as e2:
            print(f"⚠️ Fallback open failed: {e2}")

def open_folder(path):
    try:
        if sys.platform.startswith("win"):
            subprocess.Popen(["explorer", path])
        elif sys.platform == "darwin":
            subprocess.Popen(["open", path])
        else:
            subprocess.Popen(["xdg-open", path])
    except Exception as e:
        print(f"⚠️ Could not open folder: {e}")

def get_total_workbooks(server):
    """Get total count of workbooks for progress calculation"""
    try:
        req = TSC.RequestOptions()
        req.page_size = 1  # Just get count, not data
        _, pagination = server.workbooks.get(req_options=req)
        return pagination.total_available
    except Exception as e:
        print(f"⚠️ Could not get workbook count: {e}")
        return 0

def fetch_users(server):
    """Fetch all users to resolve owner names"""
    users_by_id = {}
    if not RESOLVE_OWNERS:
        return users_by_id
    
    try:
        print("👥 Fetching users for owner resolution...")
        ureq = TSC.RequestOptions()
        ureq.page_size = PAGE_SIZE
        user_count = 0
        
        while True:
            users, u_page = server.users.get(req_options=ureq)
            for u in users:
                users_by_id[u.id] = {
                    'name': u.name or u.fullname or 'Unknown',
                    'email': getattr(u, 'email', None)
                }
                user_count += 1
            
            if u_page.total_available > 0:
                print_progress_bar(user_count, u_page.total_available, 
                                 prefix='Users', suffix='fetched')
            
            if has_more(u_page):
                ureq.page_number = u_page.page_number + 1
            else:
                break
                
        print(f"✅ Fetched {len(users_by_id):,} users")
        
    except Exception as e:
        print(f"⚠️ Could not fetch users: {e}")
        users_by_id = {}
    
    return users_by_id

def get_datasource_details(wb, ds_by_id):
    """Enhanced datasource information extraction"""
    ds_info = {
        'names': [],
        'types': [],
        'urls': [],
        'servers': [],
        'databases': [],
        'table_count': 0,
        'database_count': 0
    }
    
    try:
        connections = getattr(wb, "connections", [])
        unique_servers = set()
        unique_databases = set()
        
        for c in connections:
            # Get datasource object if available
            ds_obj = ds_by_id.get(getattr(c, "datasource_id", None))
            
            if ds_obj:
                ds_info['names'].append(ds_obj.name or "")
                ds_info['types'].append(ds_obj.connection_type or "")
                ds_info['urls'].append(link_for_datasource(SERVER, SITE_ID, ds_obj.content_url))
            else:
                ds_info['names'].append("embedded/unknown")
                ds_info['types'].append(getattr(c, "connection_type", "") or "unknown")
                ds_info['urls'].append("")
            
            # Track servers and databases
            server_addr = getattr(c, "server_address", "")
            if server_addr:
                unique_servers.add(server_addr)
                ds_info['servers'].append(server_addr)
            
            db_name = getattr(c, "database_name", "")
            if db_name:
                unique_databases.add(db_name)
                ds_info['databases'].append(db_name)
        
        # Count unique databases and estimate tables (approximate)
        ds_info['database_count'] = len(unique_databases)
        # Note: Table count is difficult to get accurately without deeper inspection
        # This is an approximation based on connections
        ds_info['table_count'] = len(connections)
        
    except Exception as e:
        print(f"⚠️ Error extracting datasource details: {e}")
    
    return ds_info

def main():
    print("="*60)
    print("🚀 ENHANCED TABLEAU INVENTORY EXTRACTION")
    print("="*60)
    start_time = datetime.now()
    print(f"⏰ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    server = TSC.Server(SERVER, use_server_version=True)
    auth   = TSC.PersonalAccessTokenAuth(PAT_NAME, PAT_SECRET, SITE_ID)

    rows = []

    try:
        with server.auth.sign_in(auth):
            print(f"✅ Successfully connected to {SERVER}")
            print()

            # ---- Get total workbook count first for progress tracking ----
            print("🔍 Getting total workbook count...")
            total_workbooks = get_total_workbooks(server)
            print(f"📊 Total workbooks to process: {total_workbooks:,}")
            print()

            # ---- Fetch users for owner resolution ----
            users_by_id = fetch_users(server)
            print()

            # ---- Projects (paged) with hierarchy support ----
            print("📁 Fetching projects...")
            projects_by_id = {}
            preq = TSC.RequestOptions(); preq.page_size = PAGE_SIZE
            project_count = 0
            while True:
                projects, p_page = server.projects.get(req_options=preq)
                for p in projects:
                    projects_by_id[p.id] = {
                        'name': p.name,
                        'description': getattr(p, 'description', ''),
                        'parent_id': getattr(p, 'parent_id', None),
                        'content_permissions': getattr(p, 'content_permissions', None)
                    }
                    project_count += 1
                
                if p_page.total_available > 0:
                    print_progress_bar(project_count, p_page.total_available, 
                                     prefix='Projects', suffix='fetched')
                
                if has_more(p_page):
                    preq.page_number = p_page.page_number + 1
                else:
                    break
            print(f"✅ Fetched {len(projects_by_id):,} projects")
            print()

            # ---- Datasources (enhanced) ----
            print("💾 Fetching datasources...")
            ds_by_id = {}
            try:
                dreq = TSC.RequestOptions(); dreq.page_size = PAGE_SIZE
                ds_count = 0
                while True:
                    datasources, d_page = server.datasources.get(req_options=dreq)
                    for d in datasources:
                        ds_by_id[d.id] = d
                        ds_count += 1
                    
                    if d_page.total_available > 0:
                        print_progress_bar(ds_count, d_page.total_available, 
                                         prefix='Datasources', suffix='fetched')
                    
                    if has_more(d_page):
                        dreq.page_number = d_page.page_number + 1
                    else:
                        break
            except Exception:
                ds_by_id = {}
            print(f"✅ Fetched {len(ds_by_id):,} datasources")
            print()

            # ---- Preload usage stats for views ----
            print("📊 Fetching view usage statistics...")
            view_usage_by_id = {}
            try:
                vreq = TSC.RequestOptions()
                vreq.page_size = PAGE_SIZE
                vreq.include_usage_statistics = True
                view_count = 0
                while True:
                    views_page, v_page = server.views.get(req_options=vreq)
                    for vv in views_page:
                        view_usage_by_id[vv.id] = getattr(vv, "total_views", None)
                        view_count += 1
                    
                    if v_page.total_available > 0:
                        print_progress_bar(view_count, v_page.total_available, 
                                         prefix='View Stats', suffix='fetched')
                    
                    if has_more(v_page):
                        vreq.page_number = v_page.page_number + 1
                    else:
                        break
            except Exception:
                view_usage_by_id = {}
            print(f"✅ Fetched usage stats for {len(view_usage_by_id):,} views")
            print()

            # ---- Workbooks (paged) with enhanced metadata ----
            print("="*60)
            print("📚 PROCESSING WORKBOOKS & VIEWS")
            print("="*60)
            
            wb_req = TSC.RequestOptions(); wb_req.page_size = PAGE_SIZE
            processed_workbooks = 0
            
            while True:
                workbooks, w_page = server.workbooks.get(req_options=wb_req)

                for wb in workbooks:
                    processed_workbooks += 1
                    
                    # Update progress bar
                    if total_workbooks > 0:
                        print_progress_bar(processed_workbooks, total_workbooks, 
                                         prefix='📚 Workbooks', suffix='processed')
                    
                    # Log every 50 workbooks with better formatting
                    if processed_workbooks % 50 == 0:
                        current_total = max(actual_total, total_workbooks)
                        pct = (100*processed_workbooks/current_total) if current_total > 0 else 0
                        print(f"\n📈 Progress: {processed_workbooks:,}/{current_total:,} workbooks "
                              f"({pct:.1f}%) - Current: {wb.name[:50]}...")

                    # Populate views & connections for this workbook
                    try:
                        server.workbooks.populate_views(wb)
                    except Exception:
                        wb.views = []

                    try:
                        server.workbooks.populate_connections(wb)
                    except Exception:
                        wb.connections = []

                    # Get project info with hierarchy
                    proj_info = projects_by_id.get(wb.project_id, {})
                    proj_name = proj_info.get('name', 'Unknown')
                    
                    # Get owner info
                    owner_info = users_by_id.get(getattr(wb, "owner_id", None), {})
                    owner_name = owner_info.get('name', 'Unknown User')
                    owner_email = owner_info.get('email', '')
                    
                    wb_url = link_for_workbook(SERVER, SITE_ID, wb.content_url)

                    # Enhanced datasource information
                    ds_details = get_datasource_details(wb, ds_by_id)

                    # Process each view
                    for v in getattr(wb, "views", []):
                        v_url = link_for_view(SERVER, SITE_ID, v.content_url)
                        total_views = view_usage_by_id.get(v.id, None)

                        saved_csv_path = None
                        if DOWNLOAD_VIEW_CSV:
                            try:
                                os.makedirs(VIEW_CSV_DIR, exist_ok=True)
                                csv_bytes = server.views.populate_csv(v)
                                safe = f"{proj_name}__{wb.name}__{v.name}".replace("/", "_").replace("\\", "_")
                                saved_csv_path = os.path.join(VIEW_CSV_DIR, f"{safe}.csv")
                                with open(saved_csv_path, "wb") as f:
                                    f.write(csv_bytes)
                            except Exception as e:
                                saved_csv_path = f"ERROR: {e}"

                        rows.append({
                            # Project information
                            "project_name":         proj_name,
                            "project_id":           wb.project_id,
                            "project_description":  proj_info.get('description', ''),
                            
                            # Workbook information
                            "workbook_name":        wb.name,
                            "workbook_id":          wb.id,
                            "workbook_url":         wb_url,
                            "workbook_created":     getattr(wb, "created_at", None),
                            "workbook_updated":     getattr(wb, "updated_at", None),
                            "workbook_size":        getattr(wb, "size", None),
                            "workbook_description": getattr(wb, "description", ""),
                            
                            # Owner information
                            "owner_id":             getattr(wb, "owner_id", None),
                            "owner_name":           owner_name,
                            "owner_email":          owner_email,
                            
                            # View information
                            "view_name":            v.name,
                            "view_id":              v.id,
                            "view_url":             v_url,
                            "view_total_views":     total_views,
                            "view_created":         getattr(v, "created_at", None),
                            "view_updated":         getattr(v, "updated_at", None),
                            
                            # Enhanced datasource information
                            "datasource_names":     "; ".join(sorted(set(n for n in ds_details['names'] if n))),
                            "datasource_types":     "; ".join(sorted(set(t for t in ds_details['types'] if t))),
                            "datasource_urls":      "; ".join([u for u in ds_details['urls'] if u]),
                            "database_servers":     "; ".join(sorted(set(s for s in ds_details['servers'] if s))),
                            "database_names":       "; ".join(sorted(set(d for d in ds_details['databases'] if d))),
                            "estimated_tables":     ds_details['table_count'],
                            "databases_count":      ds_details['database_count'],
                            
                            # Optional CSV export
                            "saved_view_csv":       saved_csv_path
                        })

                if has_more(w_page):
                    wb_req.page_number = w_page.page_number + 1
                else:
                    break

    except KeyboardInterrupt:
        print(f"\n\n⚠️ Extraction interrupted by user!")
        print(f"📊 Processed {processed_workbooks:,} workbooks so far")
        if len(rows) > 0:
            print("💾 Saving partial results...")
        else:
            print("❌ No data to save")
            return

    # ---- Save & report path ----
    print(f"\n\n" + "="*60)
    print("💾 SAVING RESULTS")
    print("="*60)
    
    output_path = os.path.abspath(OUTPUT_CSV)
    df = pd.DataFrame(rows)
    
    if len(df) > 0:
        df_sorted = df.sort_values(
            ["project_name", "workbook_name", "view_name"], na_position="last"
        )
        df_sorted.to_csv(output_path, index=False)

        # Enhanced summary statistics
        total_projects = df['project_id'].nunique()
        total_workbooks_found = df['workbook_id'].nunique()
        total_views = len(df)
        total_datasources = len([ds for ds_list in df['datasource_names'] 
                               for ds in str(ds_list).split(';') if ds.strip()])
        total_owners = df['owner_name'].nunique()
        
        end_time = datetime.now()
        duration = end_time - start_time

        print("="*60)
        print("🎉 ENHANCED EXTRACTION COMPLETED!")
        print("="*60)
        print(f"📊 SUMMARY:")
        print(f"   🗂️  Projects: {total_projects:,}")
        print(f"   📚 Workbooks: {total_workbooks_found:,}")
        print(f"   📄 Views: {total_views:,}")
        print(f"   💾 Datasources: {total_datasources:,}")
        print(f"   👥 Unique Owners: {total_owners:,}")
        print(f"   ⏱️  Duration: {duration}")
        print(f"   📁 Output: {output_path}")

        if DOWNLOAD_VIEW_CSV:
            print(f"   📂 Per-view CSVs: {os.path.abspath(VIEW_CSV_DIR)}")

        # ---- Force open in Excel + open per-view folder if used ----
        print(f"\n🚀 Opening Excel...")
        open_excel(output_path)
        if DOWNLOAD_VIEW_CSV:
            open_folder(os.path.abspath(VIEW_CSV_DIR))
    else:
        print("❌ No data was extracted!")

if __name__ == "__main__":
    main()
