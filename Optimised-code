
# tableau_inventory_no_users.py
import os
import sys
import subprocess
import pandas as pd
import tableauserverclient as TSC
from datetime import datetime
import time
from collections import defaultdict
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===== EDIT THESE =====
SERVER      = "https://mytableau.cvs.com"   # Root URL (no '/#/')
SITE_ID     = "RX_OPS"                      # e.g., RX_OPS
PAT_NAME    = "xxxxxxx"
PAT_SECRET  = "xxxxxx"

OUTPUT_CSV        = "tableau_inventory_no_users.csv"
DOWNLOAD_VIEW_CSV = False                   # True = export underlying data CSVs for each view (needs permission)
VIEW_CSV_DIR      = "view_csvs"
PAGE_SIZE         = 100                     # Reduced page size to avoid timeouts
REQUEST_TIMEOUT   = 300                     # 5 minutes timeout per request
MAX_RETRIES       = 3                       # Number of retries for failed requests
RETRY_DELAY       = 5                       # Seconds to wait between retries
# ======================

def print_progress_bar(current, total, prefix='Progress', suffix='Complete', length=50, fill='‚ñà'):
    """Print a progress bar to terminal"""
    if total == 0:
        return
    percent = f"{100 * (current / float(total)):.1f}"
    filled_length = int(length * current // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix} ({current}/{total})', end='', flush=True)
    if current == total:
        print()  # New line when complete

def link_for_view(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/views/{content_url}"

def link_for_workbook(server_base, site_id, content_url):
    return f"{server_base}/#/site/{site_id}/workbooks/{content_url}"

def has_more(pagination):
    return pagination.page_number * pagination.page_size < pagination.total_available

def configure_server_with_timeout(server_url, timeout=REQUEST_TIMEOUT):
    """Configure server with proper timeout and retry settings"""
    server = TSC.Server(server_url, use_server_version=True)
    
    # Configure requests session with timeout and retries
    session = requests.Session()
    
    # Retry strategy
    retry_strategy = Retry(
        total=MAX_RETRIES,
        status_forcelist=[429, 500, 502, 503, 504],
        backoff_factor=2,
        raise_on_status=False
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Set timeout
    session.request = lambda *args, **kwargs: requests.Session.request(session, *args, timeout=timeout, **kwargs)
    
    # Apply to server
    server._session = session
    
    return server

def safe_api_call(func, *args, **kwargs):
    """Wrapper for API calls with retry logic"""
    last_exception = None
    
    for attempt in range(MAX_RETRIES + 1):
        try:
            return func(*args, **kwargs)
        except (requests.exceptions.Timeout, requests.exceptions.ReadTimeout, 
                requests.exceptions.ConnectionError, Exception) as e:
            last_exception = e
            if attempt < MAX_RETRIES:
                print(f"\n‚ö†Ô∏è API call failed (attempt {attempt + 1}/{MAX_RETRIES + 1}): {str(e)[:100]}...")
                print(f"   Retrying in {RETRY_DELAY} seconds...")
                time.sleep(RETRY_DELAY)
            else:
                print(f"\n‚ùå API call failed after {MAX_RETRIES + 1} attempts")
                raise last_exception

def open_excel(filepath):
    """Force open a file in Excel (Windows). Falls back to default opener on other OS."""
    try:
        if sys.platform.startswith("win"):
            subprocess.Popen(["excel.exe", filepath], shell=True)
        elif sys.platform == "darwin":
            subprocess.Popen(["open", "-a", "Microsoft Excel", filepath])
        else:
            subprocess.Popen(["xdg-open", filepath])
    except Exception as e:
        print(f"‚ö†Ô∏è Could not open in Excel: {e}")

def open_folder(path):
    try:
        if sys.platform.startswith("win"):
            subprocess.Popen(["explorer", path])
        elif sys.platform == "darwin":
            subprocess.Popen(["open", path])
        else:
            subprocess.Popen(["xdg-open", path])
    except Exception as e:
        print(f"‚ö†Ô∏è Could not open folder: {e}")

def fetch_paginated_data(server, endpoint_func, name, req_options=None):
    """Generic function to fetch paginated data with error handling"""
    print(f"üì• Fetching {name}...")
    
    if req_options is None:
        req_options = TSC.RequestOptions()
        req_options.page_size = PAGE_SIZE
    
    all_items = []
    item_count = 0
    total_items = 0
    
    while True:
        try:
            items, pagination = safe_api_call(endpoint_func, req_options=req_options)
            
            if total_items == 0:
                total_items = pagination.total_available
                print(f"üìä Total {name} to fetch: {total_items:,}")
            
            all_items.extend(items)
            item_count += len(items)
            
            if total_items > 0:
                print_progress_bar(item_count, total_items, prefix=name, suffix='fetched')
            
            if has_more(pagination):
                req_options.page_number = pagination.page_number + 1
                time.sleep(0.1)  # Small delay to avoid overwhelming server
            else:
                break
                
        except Exception as e:
            print(f"\n‚ùå Error fetching {name}: {e}")
            break
    
    print(f"‚úÖ Successfully fetched {len(all_items):,} {name}")
    return all_items

def extract_owner_from_workbook(wb):
    """Extract owner information from workbook object when available"""
    owner_info = {
        'owner_id': getattr(wb, 'owner_id', None),
        'owner_name': None,
        'owner_email': None
    }
    
    # Try to get owner name from workbook properties
    try:
        # Some Tableau versions include owner name in workbook
        if hasattr(wb, 'owner_name'):
            owner_info['owner_name'] = wb.owner_name
        elif hasattr(wb, '_owner_name'):
            owner_info['owner_name'] = wb._owner_name
        
        # Try to extract from XML response if available
        if hasattr(wb, '_xml_element') and wb._xml_element is not None:
            owner_element = wb._xml_element.find('.//owner')
            if owner_element is not None:
                owner_info['owner_name'] = owner_element.get('name')
                
    except Exception:
        pass
    
    # Fallback to owner_id if no name found
    if not owner_info['owner_name'] and owner_info['owner_id']:
        owner_info['owner_name'] = f"User ID: {owner_info['owner_id']}"
    
    return owner_info

def fetch_projects_safe(server):
    """Fetch projects with error handling"""
    try:
        projects = fetch_paginated_data(server, server.projects.get, "projects")
        projects_by_id = {}
        for project in projects:
            projects_by_id[project.id] = project
        return projects_by_id
    except Exception as e:
        print(f"‚ö†Ô∏è Could not fetch projects: {e}")
        return {}

def fetch_datasources_safe(server):
    """Fetch datasources with error handling"""
    try:
        datasources = fetch_paginated_data(server, server.datasources.get, "datasources")
        ds_by_id = {}
        for ds in datasources:
            ds_by_id[ds.id] = ds
        return ds_by_id
    except Exception as e:
        print(f"‚ö†Ô∏è Could not fetch datasources: {e}")
        return {}

def fetch_workbooks_safe(server):
    """Fetch workbooks with error handling"""
    try:
        workbooks = fetch_paginated_data(server, server.workbooks.get, "workbooks")
        return workbooks
    except Exception as e:
        print(f"‚ö†Ô∏è Could not fetch workbooks: {e}")
        return []

def get_workbook_owner_names_from_views(server, workbook_ids):
    """Try to get owner names by looking at view ownership"""
    print("üîç Attempting to get owner info from views...")
    owner_cache = {}
    
    try:
        # Get all views with usage statistics (might include owner info)
        req = TSC.RequestOptions()
        req.page_size = PAGE_SIZE
        req.include_usage_statistics = True
        
        view_count = 0
        while True:
            try:
                views, pagination = safe_api_call(server.views.get, req_options=req)
                
                for view in views:
                    wb_id = getattr(view, 'workbook_id', None)
                    if wb_id in workbook_ids and wb_id not in owner_cache:
                        # Try to extract owner info from view
                        if hasattr(view, '_xml_element') and view._xml_element is not None:
                            wb_element = view._xml_element.find('.//workbook')
                            if wb_element is not None:
                                owner_element = wb_element.find('.//owner')
                                if owner_element is not None:
                                    owner_cache[wb_id] = owner_element.get('name', f"User ID: {getattr(view, 'owner_id', 'Unknown')}")
                
                view_count += len(views)
                print_progress_bar(view_count, pagination.total_available if pagination.total_available > 0 else view_count, 
                                 prefix='Views analyzed', suffix='for owners')
                
                if has_more(pagination):
                    req.page_number = pagination.page_number + 1
                    time.sleep(0.1)
                else:
                    break
                    
            except Exception as e:
                print(f"\n‚ö†Ô∏è Error getting views for owner info: {e}")
                break
                
    except Exception as e:
        print(f"‚ö†Ô∏è Could not get owner info from views: {e}")
    
    print(f"‚úÖ Found owner names for {len(owner_cache)} workbooks from views")
    return owner_cache

def process_single_workbook(server, wb, projects_by_id, ds_by_id, owner_cache=None):
    """Process a single workbook with error handling"""
    rows = []
    
    try:
        # Get basic workbook info
        proj = projects_by_id.get(wb.project_id)
        
        # Get owner info - try multiple methods
        owner_info = extract_owner_from_workbook(wb)
        
        # Use cached owner info if available
        if owner_cache and wb.id in owner_cache:
            owner_info['owner_name'] = owner_cache[wb.id]
        
        wb_url = link_for_workbook(SERVER, SITE_ID, wb.content_url)
        
        # Safely populate views
        try:
            safe_api_call(server.workbooks.populate_views, wb)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get views for workbook '{wb.name}': {e}")
            wb.views = []

        # Safely populate connections
        try:
            safe_api_call(server.workbooks.populate_connections, wb)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not get connections for workbook '{wb.name}': {e}")
            wb.connections = []

        # Process datasource info
        ds_names, ds_types = [], []
        for c in getattr(wb, "connections", []):
            ds_obj = ds_by_id.get(getattr(c, "datasource_id", None))
            if ds_obj:
                ds_names.append(ds_obj.name or "")
                ds_types.append(ds_obj.connection_type or "")
            else:
                ds_names.append(getattr(c, "server_address", "") or "embedded/unknown")
                ds_types.append(getattr(c, "connection_type", "") or "unknown")

        # Process views
        views = getattr(wb, "views", [])
        if views:
            for v in views:
                try:
                    v_url = link_for_view(SERVER, SITE_ID, v.content_url)
                    
                    # Get view usage stats safely
                    total_views = None
                    try:
                        total_views = getattr(v, "total_views", None)
                    except:
                        pass

                    saved_csv_path = None
                    if DOWNLOAD_VIEW_CSV:
                        try:
                            os.makedirs(VIEW_CSV_DIR, exist_ok=True)
                            csv_bytes = safe_api_call(server.views.populate_csv, v)
                            safe = f"{(proj.name if proj else 'NoProject')}__{wb.name}__{v.name}".replace("/", "_").replace("\\", "_")
                            saved_csv_path = os.path.join(VIEW_CSV_DIR, f"{safe}.csv")
                            with open(saved_csv_path, "wb") as f:
                                f.write(csv_bytes)
                        except Exception as e:
                            saved_csv_path = f"ERROR: {e}"

                    rows.append({
                        "project_name":      proj.name if proj else None,
                        "project_id":        wb.project_id,
                        "workbook_name":     wb.name,
                        "workbook_id":       wb.id,
                        "workbook_url":      wb_url,
                        "workbook_created":  getattr(wb, "created_at", None),
                        "workbook_updated":  getattr(wb, "updated_at", None),
                        "owner_id":          owner_info['owner_id'],
                        "owner_name":        owner_info['owner_name'] or "Permission Denied",
                        "owner_email":       owner_info['owner_email'],
                        "view_name":         v.name,
                        "view_id":           v.id,
                        "view_url":          v_url,
                        "view_total_views":  total_views,
                        "datasource_names":  "; ".join(sorted(set(n for n in ds_names if n))),
                        "datasource_types":  "; ".join(sorted(set(t for t in ds_types if t))),
                        "saved_view_csv":    saved_csv_path
                    })
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing view '{v.name}' in workbook '{wb.name}': {e}")
        else:
            # Workbook with no views
            rows.append({
                "project_name":      proj.name if proj else None,
                "project_id":        wb.project_id,
                "workbook_name":     wb.name,
                "workbook_id":       wb.id,
                "workbook_url":      wb_url,
                "workbook_created":  getattr(wb, "created_at", None),
                "workbook_updated":  getattr(wb, "updated_at", None),
                "owner_id":          owner_info['owner_id'],
                "owner_name":        owner_info['owner_name'] or "Permission Denied",
                "owner_email":       owner_info['owner_email'],
                "view_name":         None,
                "view_id":           None,
                "view_url":          None,
                "view_total_views":  None,
                "datasource_names":  "; ".join(sorted(set(n for n in ds_names if n))),
                "datasource_types":  "; ".join(sorted(set(t for t in ds_types if t))),
                "saved_view_csv":    None
            })

    except Exception as e:
        print(f"‚ö†Ô∏è Error processing workbook '{wb.name}': {e}")
        # Still try to capture basic workbook info
        try:
            proj = projects_by_id.get(wb.project_id)
            owner_info = extract_owner_from_workbook(wb)
            wb_url = link_for_workbook(SERVER, SITE_ID, wb.content_url)
            
            rows.append({
                "project_name":      proj.name if proj else None,
                "project_id":        wb.project_id,
                "workbook_name":     wb.name,
                "workbook_id":       wb.id,
                "workbook_url":      wb_url,
                "workbook_created":  getattr(wb, "created_at", None),
                "workbook_updated":  getattr(wb, "updated_at", None),
                "owner_id":          owner_info['owner_id'],
                "owner_name":        owner_info['owner_name'] or f"ERROR: {str(e)[:50]}",
                "owner_email":       owner_info['owner_email'],
                "view_name":         f"ERROR: {str(e)[:100]}",
                "view_id":           None,
                "view_url":          None,
                "view_total_views":  None,
                "datasource_names":  "ERROR",
                "datasource_types":  "ERROR",
                "saved_view_csv":    None
            })
        except:
            pass
    
    return rows

def main():
    print("="*60)
    print("üõ°Ô∏è TABLEAU INVENTORY (NO USER PERMISSIONS REQUIRED)")
    print("="*60)
    print(f"‚öôÔ∏è Configuration:")
    print(f"   üìÑ Page Size: {PAGE_SIZE}")
    print(f"   ‚è±Ô∏è  Request Timeout: {REQUEST_TIMEOUT}s")
    print(f"   üîÑ Max Retries: {MAX_RETRIES}")
    print(f"   ‚è≥ Retry Delay: {RETRY_DELAY}s")
    print(f"   üë• User Lookup: DISABLED (no permissions)")
    print("="*60)
    
    start_time = datetime.now()
    print(f"‚è∞ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    server = configure_server_with_timeout(SERVER)
    auth = TSC.PersonalAccessTokenAuth(PAT_NAME, PAT_SECRET, SITE_ID)

    all_rows = []
    processed_workbooks = 0

    try:
        with server.auth.sign_in(auth):
            print(f"‚úÖ Successfully connected to {SERVER}")
            print()

            # Fetch reference data (skip users due to permission issue)
            print("üîÑ Fetching reference data...")
            projects_by_id = fetch_projects_safe(server)
            ds_by_id = fetch_datasources_safe(server)
            workbooks = fetch_workbooks_safe(server)
            
            print(f"\nüìä Summary of fetched data:")
            print(f"   üóÇÔ∏è  Projects: {len(projects_by_id):,}")
            print(f"   üë• Users: SKIPPED (no permissions)")
            print(f"   üíæ Datasources: {len(ds_by_id):,}")
            print(f"   üìö Workbooks: {len(workbooks):,}")
            
            if not workbooks:
                print("‚ùå No workbooks found. Exiting.")
                return
            
            # Try to get owner names from views (alternative approach)
            workbook_ids = {wb.id for wb in workbooks}
            owner_cache = get_workbook_owner_names_from_views(server, workbook_ids)
            
            # Process workbooks one by one
            print("\n" + "="*60)
            print("üìö PROCESSING WORKBOOKS")
            print("="*60)
            
            total_workbooks = len(workbooks)
            
            for i, wb in enumerate(workbooks, 1):
                processed_workbooks = i
                print_progress_bar(i, total_workbooks, 
                                 prefix='üìö Workbooks', suffix='processed')
                
                # Log progress every 10 workbooks
                if i % 10 == 0:
                    print(f"\nüìà Progress: {i:,}/{total_workbooks:,} workbooks "
                          f"({100*i/total_workbooks:.1f}%) - Current: {wb.name[:50]}...")

                wb_rows = process_single_workbook(server, wb, projects_by_id, ds_by_id, owner_cache)
                all_rows.extend(wb_rows)
                
                # Small delay to avoid overwhelming server
                time.sleep(0.1)

    except KeyboardInterrupt:
        print(f"\n\n‚ö†Ô∏è Extraction interrupted by user!")
        print(f"üìä Processed {processed_workbooks:,} workbooks so far")
        if len(all_rows) > 0:
            print("üíæ Saving partial results...")
        else:
            print("‚ùå No data to save")
            return

    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        print(f"üìä Processed {processed_workbooks:,} workbooks so far")
        if len(all_rows) > 0:
            print("üíæ Saving partial results...")
        else:
            print("‚ùå No data to save")
            return

    # Save results
    print(f"\n\n" + "="*60)
    print("üíæ SAVING RESULTS")
    print("="*60)
    
    output_path = os.path.abspath(OUTPUT_CSV)
    
    if len(all_rows) > 0:
        df = pd.DataFrame(all_rows)
        df_sorted = df.sort_values(
            ["project_name", "workbook_name", "view_name"], na_position="last"
        )
        df_sorted.to_csv(output_path, index=False)

        # Summary statistics
        total_projects = df['project_id'].nunique()
        total_workbooks_found = df['workbook_id'].nunique()
        total_views = len(df[df['view_id'].notna()])
        
        # Count how many workbooks have owner names vs just IDs
        owner_names_found = len(df[(df['owner_name'].notna()) & 
                                 (~df['owner_name'].str.startswith('User ID:')) & 
                                 (df['owner_name'] != 'Permission Denied')]['workbook_id'].unique())
        
        end_time = datetime.now()
        duration = end_time - start_time

        print("="*60)
        print("üéâ EXTRACTION COMPLETED!")
        print("="*60)
        print(f"üìä SUMMARY:")
        print(f"   üóÇÔ∏è  Projects: {total_projects:,}")
        print(f"   üìö Workbooks: {total_workbooks_found:,}")
        print(f"   üìÑ Views: {total_views:,}")
        print(f"   üë§ Owner Names Found: {owner_names_found:,}/{total_workbooks_found:,}")
        print(f"   ‚è±Ô∏è  Duration: {duration}")
        print(f"   üìÅ Output: {output_path}")
        
        if owner_names_found < total_workbooks_found:
            print(f"\nüìù NOTE: Some owner names show as 'User ID: xxx' or 'Permission Denied'")
            print(f"   This is due to insufficient permissions to query user details.")

        if DOWNLOAD_VIEW_CSV:
            print(f"   üìÇ Per-view CSVs: {os.path.abspath(VIEW_CSV_DIR)}")

        # Open in Excel
        print(f"\nüöÄ Opening Excel...")
        open_excel(output_path)
        if DOWNLOAD_VIEW_CSV:
            open_folder(os.path.abspath(VIEW_CSV_DIR))
    else:
        print("‚ùå No data was extracted!")

if __name__ == "__main__":
    main()
